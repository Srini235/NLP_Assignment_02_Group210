{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploring Attention Mechanisms and Contextual Embeddings for Part-of-Speech Tagging\n",
        "\n",
        "**Author:** _Your Name_\n",
        "\n",
        "This notebook implements the full assignment pipeline:\n",
        "\n",
        "- **Task 1:** Dataset exploration\n",
        "- **Task 2:** Baseline POS tagger (static/non-contextual embeddings + BiLSTM)\n",
        "- **Task 3:** Contextual embeddings (BERT) for token classification\n",
        "- **Task 4:** Attention-based POS tagger (BiLSTM + additive attention) + visualizations\n",
        "- **Task 5:** Comparative analysis (Accuracy, Precision, Recall, F1, Complexity)\n",
        "\n",
        "> Recommended environment: Python 3.10+ with a CUDA-enabled GPU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Setup\n",
        "Run the cell below to install required packages (skip if already installed)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# If running on Colab/Kaggle, you may uncomment the following lines.\n",
        "# Choose the appropriate PyTorch CUDA index-url for your environment or simply use the default CPU install.\n",
        "!pip -q install torch torchvision torchaudio\n",
        "!pip -q install transformers datasets seqeval scikit-learn matplotlib seaborn tqdm conllu\n",
        "# Optional for ELMo (not used by default):\n",
        "# !pip -q install allennlp allennlp-models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Imports & Global Config\n",
        "This sets deterministic seeds, device, and common utilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, random, statistics, math\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Reproducibility\n",
        "SEED = 42\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('Device:', device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìÇ Data: Universal Dependencies (UD) English\n",
        "\n",
        "This notebook expects three CoNLL-U files under `data/ud_en/`:\n",
        "\n",
        "- `train.conllu`\n",
        "- `dev.conllu`\n",
        "- `test.conllu`\n",
        "\n",
        "You can:\n",
        "1. **Place the Kaggle UDOS dataset files** in that folder and rename to the above, _or_\n",
        "2. **Auto-download** the UD English EWT (r2.14) from the official UD GitHub (next cell, optional).\n",
        "\n",
        "> CoNLL-U fields: `ID FORM LEMMA UPOS XPOS FEATS HEAD DEPREL DEPS MISC` ‚Äî we use `FORM` and `UPOS`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Auto-download UD English EWT r2.14 from official repo\n",
        "# If you already have the Kaggle files, skip this cell.\n",
        "import os, requests\n",
        "from pathlib import Path\n",
        "\n",
        "DATA_DIR = Path('data/ud_en')\n",
        "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "base = 'https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/r2.14'\n",
        "files = {\n",
        "    'train.conllu': 'en_ewt-ud-train.conllu',\n",
        "    'dev.conllu':   'en_ewt-ud-dev.conllu',\n",
        "    'test.conllu':  'en_ewt-ud-test.conllu',\n",
        "}\n",
        "\n",
        "for out_name, src_name in files.items():\n",
        "    out_path = DATA_DIR/out_name\n",
        "    if not out_path.exists():\n",
        "        url = f\"{base}/{src_name}\"\n",
        "        print('Downloading', url)\n",
        "        r = requests.get(url)\n",
        "        r.raise_for_status()\n",
        "        out_path.write_bytes(r.content)\n",
        "\n",
        "print('Data files present:', list(DATA_DIR.iterdir()))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 1 ‚Äî Dataset Exploration (2 marks)\n",
        "We load the UD dataset, compute basic statistics, show tag distribution, and display sample sentences with tags."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from conllu import parse_incr\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "DATA_DIR = Path('data/ud_en')\n",
        "TRAIN = DATA_DIR/'train.conllu'\n",
        "DEV   = DATA_DIR/'dev.conllu'\n",
        "TEST  = DATA_DIR/'test.conllu'\n",
        "assert TRAIN.exists() and DEV.exists() and TEST.exists(), 'Missing .conllu files in data/ud_en/'\n",
        "\n",
        "\n",
        "def read_conllu(path):\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for sent in parse_incr(f):\n",
        "            tokens = [t['form'] for t in sent if isinstance(t['id'], int)]\n",
        "            tags   = [t['upostag'] for t in sent if isinstance(t['id'], int)]\n",
        "            yield tokens, tags\n",
        "\n",
        "train = list(read_conllu(TRAIN))\n",
        "dev   = list(read_conllu(DEV))\n",
        "test  = list(read_conllu(TEST))\n",
        "\n",
        "len_train, len_dev, len_test = len(train), len(dev), len(test)\n",
        "print(f\"Train={len_train}, Dev={len_dev}, Test={len_test}\")\n",
        "\n",
        "all_sents = train + dev + test\n",
        "num_sentences = len(all_sents)\n",
        "num_tokens = sum(len(s[0]) for s in all_sents)\n",
        "avg_len = num_tokens / num_sentences\n",
        "\n",
        "print(f\"Total sentences: {num_sentences}\")\n",
        "print(f\"Total tokens: {num_tokens}\")\n",
        "print(f\"Average sentence length: {avg_len:.2f}\")\n",
        "\n",
        "tag_counter = Counter(tag for _, tags in all_sents for tag in tags)\n",
        "print(\"Top tags:\")\n",
        "for tag, c in tag_counter.most_common(20):\n",
        "    print(f\"{tag:>5}: {c}\")\n",
        "\n",
        "# Show a few random samples\n",
        "for i in range(3):\n",
        "    tokens, tags = random.choice(all_sents)\n",
        "    print('Tokens :', tokens)\n",
        "    print('UPOS   :', tags)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Shared Utilities for Sequence Tagging\n",
        "We build vocabularies, PyTorch datasets and loaders, and evaluation helpers (SeqEval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "PAD_TOKEN = '<PAD>'\n",
        "UNK_TOKEN = '<UNK>'\n",
        "\n",
        "\n",
        "def build_vocab(sentences, min_freq=1):\n",
        "    wc = Counter(tok.lower() for toks, _ in sentences for tok in toks)\n",
        "    itos = [PAD_TOKEN, UNK_TOKEN] + [w for w, c in wc.items() if c >= min_freq]\n",
        "    stoi = {w:i for i, w in enumerate(itos)}\n",
        "    return stoi, itos\n",
        "\n",
        "\n",
        "def build_tag_map(sentences):\n",
        "    tags = sorted({t for _, ts in sentences for t in ts})\n",
        "    tag2id = {t:i for i, t in enumerate(tags)}\n",
        "    id2tag = {i:t for t, i in tag2id.items()}\n",
        "    return tag2id, id2tag\n",
        "\n",
        "\n",
        "word2id, id2word = build_vocab(train)\n",
        "tag2id, id2tag   = build_tag_map(train)\n",
        "PAD_ID = word2id[PAD_TOKEN]\n",
        "\n",
        "\n",
        "MAX_LEN = 128\n",
        "\n",
        "\n",
        "def vectorize(tokens, tags=None, max_len=MAX_LEN):\n",
        "    ids = [word2id.get(tok.lower(), word2id[UNK_TOKEN]) for tok in tokens]\n",
        "    tids = None\n",
        "    if tags is not None:\n",
        "        tids = [tag2id[t] for t in tags]\n",
        "    if max_len is not None:\n",
        "        ids = ids[:max_len]\n",
        "        if tids is not None:\n",
        "            tids = tids[:max_len]\n",
        "    return ids, tids\n",
        "\n",
        "\n",
        "class SeqDataset(Dataset):\n",
        "    def __init__(self, samples, max_len=MAX_LEN):\n",
        "        self.samples = samples\n",
        "        self.max_len = max_len\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        tokens, tags = self.samples[idx]\n",
        "        wi, ti = vectorize(tokens, tags, self.max_len)\n",
        "        return wi, ti\n",
        "\n",
        "\n",
        "def collate(batch):\n",
        "    xs, ys = zip(*batch)\n",
        "    max_len = max(len(x) for x in xs)\n",
        "    px = [x + [PAD_ID]*(max_len-len(x)) for x in xs]\n",
        "    py = [y + [-100]*(max_len-len(y)) for y in ys]  # -100 ignored in CrossEntropyLoss\n",
        "    attn = [[1]*len(x) + [0]*(max_len-len(x)) for x in xs]\n",
        "    return (torch.tensor(px, dtype=torch.long),\n",
        "            torch.tensor(py, dtype=torch.long),\n",
        "            torch.tensor(attn, dtype=torch.bool))\n",
        "\n",
        "\n",
        "train_dl = DataLoader(SeqDataset(train), batch_size=64, shuffle=True, collate_fn=collate)\n",
        "dev_dl   = DataLoader(SeqDataset(dev),   batch_size=128, shuffle=False, collate_fn=collate)\n",
        "test_dl  = DataLoader(SeqDataset(test),  batch_size=128, shuffle=False, collate_fn=collate)\n",
        "\n",
        "\n",
        "def evaluate_sequences(true_ids, pred_ids):\n",
        "    all_true, all_pred = [], []\n",
        "    for y, p in zip(true_ids, pred_ids):\n",
        "        y_tags = [id2tag[i] for i in y]\n",
        "        p_tags = [id2tag[i] for i in p]\n",
        "        all_true.append(y_tags)\n",
        "        all_pred.append(p_tags)\n",
        "    acc = accuracy_score(all_true, all_pred)\n",
        "    macro_f1 = f1_score(all_true, all_pred, average='macro')\n",
        "    micro_f1 = f1_score(all_true, all_pred, average='micro')\n",
        "    report = classification_report(all_true, all_pred, digits=4)\n",
        "    return acc, macro_f1, micro_f1, report\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 2 ‚Äî Baseline POS Tagger (Static Embeddings + BiLSTM) (2 marks)\n",
        "We train a BiLSTM tagger with a trainable embedding. Loss ignores padding tokens, metrics use `seqeval`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BiLSTMTagger(nn.Module):\n",
        "    def __init__(self, vocab_size, num_tags, emb_dim=100, hidden_dim=256, num_layers=1, dropout=0.2, pad_idx=0):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim//2, num_layers=num_layers,\n",
        "                            batch_first=True, dropout=dropout if num_layers>1 else 0,\n",
        "                            bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.fc = nn.Linear(hidden_dim, num_tags)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        e = self.emb(x)                     # (B, T, E)\n",
        "        out, _ = self.lstm(e)               # (B, T, H)\n",
        "        out = self.dropout(out)\n",
        "        logits = self.fc(out)               # (B, T, C)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def train_epoch(model, dl, optimizer, criterion):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for X, Y, M in dl:\n",
        "        X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X, M)\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(dl))\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model(model, dl):\n",
        "    model.eval()\n",
        "    all_true, all_pred = [], []\n",
        "    for X, Y, M in dl:\n",
        "        X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
        "        logits = model(X, M)\n",
        "        pred = logits.argmax(-1)\n",
        "        for y, p, m in zip(Y, pred, M):\n",
        "            y = y[m].tolist()\n",
        "            p = p[m].tolist()\n",
        "            all_true.append(y)\n",
        "            all_pred.append(p)\n",
        "    acc, macro_f1, micro_f1, report = evaluate_sequences(all_true, all_pred)\n",
        "    return acc, macro_f1, micro_f1, report\n",
        "\n",
        "model_baseline = BiLSTMTagger(vocab_size=len(id2word), num_tags=len(tag2id), pad_idx=PAD_ID).to(device)\n",
        "optimizer = torch.optim.AdamW(model_baseline.parameters(), lr=3e-3, weight_decay=1e-4)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "EPOCHS = 8\n",
        "for epoch in range(EPOCHS):\n",
        "    tr_loss = train_epoch(model_baseline, train_dl, optimizer, criterion)\n",
        "    acc, mF1, uF1, rep = evaluate_model(model_baseline, dev_dl)\n",
        "    print(f\"Epoch {epoch+1:02d} | train_loss={tr_loss:.4f} | dev_acc={acc:.4f} | dev_macroF1={mF1:.4f}\")\n",
        "\n",
        "print(\"\n",
        "DEV REPORT (Baseline):\n",
        "\", rep)\n",
        "acc_t, mF1_t, uF1_t, rep_t = evaluate_model(model_baseline, test_dl)\n",
        "print(\"TEST ACC:\", acc_t)\n",
        "print(\"TEST REPORT (Baseline):\n",
        "\", rep_t)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 3 ‚Äî Contextual Embedding-Based POS Tagger (BERT) (2 marks)\n",
        "We fine-tune **BERT-base** for token classification. We align UD tokens to WordPiece and label the **first subword** only, masking the rest (`-100`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForTokenClassification, DataCollatorForTokenClassification, TrainingArguments, Trainer\n",
        "\n",
        "MODEL_NAME = 'bert-base-cased'  # alternatives: 'roberta-base', 'bert-base-uncased'\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "label_list = [id2tag[i] for i in range(len(id2tag))]\n",
        "label2id = {l:i for i,l in enumerate(label_list)}\n",
        "id2label = {i:l for l,i in label2id.items()}\n",
        "\n",
        "\n",
        "def to_hf_dict(sents):\n",
        "    return {\"tokens\":[t for t,_ in sents], \"tags\":[u for _,u in sents]}\n",
        "\n",
        "train_hf, dev_hf, test_hf = to_hf_dict(train), to_hf_dict(dev), to_hf_dict(test)\n",
        "\n",
        "\n",
        "def encode_dataset(data):\n",
        "    enc = tokenizer(data['tokens'], is_split_into_words=True, truncation=True, padding=False, max_length=128)\n",
        "    all_labels = []\n",
        "    for i in range(len(data['tokens'])):\n",
        "        word_ids = enc.word_ids(batch_index=i)\n",
        "        labels = data['tags'][i]\n",
        "        aligned = []\n",
        "        prev_wid = None\n",
        "        for wid in word_ids:\n",
        "            if wid is None:\n",
        "                aligned.append(-100)\n",
        "            else:\n",
        "                if wid != prev_wid:\n",
        "                    aligned.append(label2id[labels[wid]])\n",
        "                else:\n",
        "                    aligned.append(-100)\n",
        "                prev_wid = wid\n",
        "        all_labels.append(aligned)\n",
        "    enc['labels'] = all_labels\n",
        "    return enc\n",
        "\n",
        "train_enc = encode_dataset(train_hf)\n",
        "dev_enc   = encode_dataset(dev_hf)\n",
        "test_enc  = encode_dataset(test_hf)\n",
        "\n",
        "class HFTokenDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, enc):\n",
        "        self.enc = enc\n",
        "    def __len__(self): return len(self.enc['input_ids'])\n",
        "    def __getitem__(self, idx):\n",
        "        return {k: torch.tensor(v[idx]) for k,v in self.enc.items()}\n",
        "\n",
        "train_ds = HFTokenDataset(train_enc)\n",
        "dev_ds   = HFTokenDataset(dev_enc)\n",
        "test_ds  = HFTokenDataset(test_enc)\n",
        "\n",
        "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n",
        "\n",
        "model_ctx = AutoModelForTokenClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=len(label_list),\n",
        "    id2label=id2label,\n",
        "    label2id=label2id\n",
        ").to(device)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir='outputs/bert-pos',\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=4,\n",
        "    weight_decay=0.01,\n",
        "    evaluation_strategy='epoch',\n",
        "    save_strategy='epoch',\n",
        "    logging_steps=50,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=[]\n",
        ")\n",
        "\n",
        "# We'll compute only accuracy inside Trainer and do a full report later.\n",
        "import numpy as np\n",
        "from seqeval.metrics import accuracy_score as seqeval_accuracy\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    true_seqs, pred_seqs = [], []\n",
        "    for p, l in zip(preds, labels):\n",
        "        y, yh = [], []\n",
        "        for pi, li in zip(p, l):\n",
        "            if li != -100:\n",
        "                y.append(id2label[int(li)])\n",
        "                yh.append(id2label[int(pi)])\n",
        "        true_seqs.append(y)\n",
        "        pred_seqs.append(yh)\n",
        "    return {\"accuracy\": seqeval_accuracy(true_seqs, pred_seqs)}\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model_ctx,\n",
        "    args=args,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=dev_ds,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics_dev = trainer.evaluate()\n",
        "print('DEV metrics:', metrics_dev)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full test set evaluation with detailed report\n",
        "from seqeval.metrics import classification_report, accuracy_score, f1_score\n",
        "\n",
        "pred_output = trainer.predict(test_ds)\n",
        "logits = pred_output.predictions\n",
        "labels = pred_output.label_ids\n",
        "preds = np.argmax(logits, axis=-1)\n",
        "\n",
        "true_seqs, pred_seqs = [], []\n",
        "for p, l in zip(preds, labels):\n",
        "    y, yh = [], []\n",
        "    for pi, li in zip(p, l):\n",
        "        if li != -100:\n",
        "            y.append(id2label[int(li)])\n",
        "            yh.append(id2label[int(pi)])\n",
        "    true_seqs.append(y)\n",
        "    pred_seqs.append(yh)\n",
        "\n",
        "print('BERT Test Accuracy:', accuracy_score(true_seqs, pred_seqs))\n",
        "print('BERT Test macro-F1:', f1_score(true_seqs, pred_seqs, average='macro'))\n",
        "print('BERT Test micro-F1:', f1_score(true_seqs, pred_seqs, average='micro'))\n",
        "print('\n",
        "BERT TEST REPORT:\n",
        "', classification_report(true_seqs, pred_seqs, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 4 ‚Äî Attention-Based POS Tagger (BiLSTM + Additive Attention) (2 marks)\n",
        "We add token-to-context **additive attention** over BiLSTM outputs and visualize attention maps for ambiguous examples.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "class BiLSTMWithAttention(nn.Module):\n",
        "    def __init__(self, vocab_size, num_tags, emb_dim=100, hidden_dim=256, pad_idx=0, dropout=0.2):\n",
        "        super().__init__()\n",
        "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=pad_idx)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim//2, batch_first=True, bidirectional=True)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.W_q = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.W_k = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v   = nn.Linear(hidden_dim, 1, bias=False)\n",
        "        self.fc  = nn.Linear(hidden_dim*2, num_tags)  # concat [h_t ; context_t]\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        h, _ = self.lstm(self.emb(x))             # (B, T, H)\n",
        "        h = self.dropout(h)\n",
        "        Q = self.W_q(h)                            # (B, T, H)\n",
        "        K = self.W_k(h)                            # (B, T, H)\n",
        "\n",
        "        B, T, H = Q.size()\n",
        "        Qe = Q.unsqueeze(2).expand(B, T, T, H)\n",
        "        Ke = K.unsqueeze(1).expand(B, T, T, H)\n",
        "        scores = self.v(torch.tanh(Qe + Ke)).squeeze(-1)  # (B, T, T)\n",
        "\n",
        "        key_mask = mask.unsqueeze(1).expand(B, T, T)      # (B, T, T)\n",
        "        scores = scores.masked_fill(~key_mask, -1e9)\n",
        "\n",
        "        attn = torch.softmax(scores, dim=-1)              # (B, T, T)\n",
        "        context = attn @ h                                 # (B, T, H)\n",
        "\n",
        "        out = torch.cat([h, context], dim=-1)             # (B, T, 2H)\n",
        "        logits = self.fc(self.dropout(out))               # (B, T, C)\n",
        "        return logits, attn\n",
        "\n",
        "\n",
        "def train_epoch_attn(model, dl, optimizer, criterion):\n",
        "    model.train()\n",
        "    total = 0.0\n",
        "    for X, Y, M in dl:\n",
        "        X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits, attn = model(X, M)\n",
        "        loss = criterion(logits.view(-1, logits.size(-1)), Y.view(-1))\n",
        "        loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "        total += loss.item()\n",
        "    return total / max(1, len(dl))\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_model_attn(model, dl):\n",
        "    model.eval()\n",
        "    all_true, all_pred = [], []\n",
        "    for X, Y, M in dl:\n",
        "        X, Y, M = X.to(device), Y.to(device), M.to(device)\n",
        "        logits, attn = model(X, M)\n",
        "        pred = logits.argmax(-1)\n",
        "        for y, p, m in zip(Y, pred, M):\n",
        "            y = y[m].tolist()\n",
        "            p = p[m].tolist()\n",
        "            all_true.append(y)\n",
        "            all_pred.append(p)\n",
        "    acc, macro_f1, micro_f1, report = evaluate_sequences(all_true, all_pred)\n",
        "    return acc, macro_f1, micro_f1, report\n",
        "\n",
        "model_attn = BiLSTMWithAttention(vocab_size=len(id2word), num_tags=len(tag2id), pad_idx=PAD_ID).to(device)\n",
        "opt_attn = torch.optim.AdamW(model_attn.parameters(), lr=2e-3, weight_decay=1e-4)\n",
        "crit = nn.CrossEntropyLoss(ignore_index=-100)\n",
        "\n",
        "EPOCHS_ATTN = 6\n",
        "for epoch in range(EPOCHS_ATTN):\n",
        "    tr_loss = train_epoch_attn(model_attn, train_dl, opt_attn, crit)\n",
        "    acc, mF1, uF1, rep = evaluate_model_attn(model_attn, dev_dl)\n",
        "    print(f\"Epoch {epoch+1:02d} | train_loss={tr_loss:.4f} | dev_acc={acc:.4f} | dev_macroF1={mF1:.4f}\")\n",
        "\n",
        "print(\"\n",
        "DEV REPORT (BiLSTM+Attention):\n",
        "\", rep)\n",
        "acc_t2, mF1_t2, uF1_t2, rep_t2 = evaluate_model_attn(model_attn, test_dl)\n",
        "print(\"TEST ACC:\", acc_t2)\n",
        "print(\"TEST REPORT (BiLSTM+Attention):\n",
        "\", rep_t2)\n",
        "\n",
        "\n",
        "@torch.no_grad()\n",
        "def visualize_attention(model, sentence_tokens):\n",
        "    model.eval()\n",
        "    wi, _ = vectorize(sentence_tokens, None, max_len=MAX_LEN)\n",
        "    X = torch.tensor([wi], dtype=torch.long).to(device)\n",
        "    M = torch.tensor([[1]*len(sentence_tokens)], dtype=torch.bool).to(device)\n",
        "    logits, attn = model(X, M)\n",
        "    T = len(sentence_tokens)\n",
        "    mat = attn[0, :T, :T].detach().cpu().numpy()\n",
        "    plt.figure(figsize=(min(0.6*T+2, 12), min(0.6*T+2, 12)))\n",
        "    sns.heatmap(mat, xticklabels=sentence_tokens, yticklabels=sentence_tokens, cmap='viridis')\n",
        "    plt.title('Token-to-Context Attention Heatmap')\n",
        "    plt.xlabel('Context tokens (keys)')\n",
        "    plt.ylabel('Query tokens')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Examples with ambiguity\n",
        "examples = [\n",
        "    [\"I\",\"saw\",\"a\",\"duck\",\"today\",\".\"],\n",
        "    [\"Please\",\"record\",\"the\",\"meeting\",\".\"],\n",
        "    [\"I\",\"will\",\"book\",\"a\",\"table\",\".\"]\n",
        "]\n",
        "for ex in examples:\n",
        "    visualize_attention(model_attn, ex)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Task 5 ‚Äî Comparative Analysis (2 marks)\n",
        "We summarize performance and discuss computational complexity. Fill in the table with your results from the runs above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Enter collected metrics from previous cells (or keep the defaults updated programmatically)\n",
        "res = []\n",
        "# Baseline (filled from variables if available)\n",
        "try:\n",
        "    res.append({\n",
        "        'Model': 'BiLSTM (Static Emb)',\n",
        "        'Accuracy': acc_t,\n",
        "        'Macro F1': mF1_t,\n",
        "        'Micro F1': uF1_t,\n",
        "        'Params (~)': '5‚Äì10M',\n",
        "        'Complexity': 'O(T) recurrent'\n",
        "    })\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "# Attention\n",
        "try:\n",
        "    res.append({\n",
        "        'Model': 'BiLSTM + Additive Attention',\n",
        "        'Accuracy': acc_t2,\n",
        "        'Macro F1': mF1_t2,\n",
        "        'Micro F1': uF1_t2,\n",
        "        'Params (~)': '6‚Äì12M',\n",
        "        'Complexity': 'O(T^2) attention'\n",
        "    })\n",
        "except NameError:\n",
        "    pass\n",
        "\n",
        "# BERT (we computed in the BERT test cell)\n",
        "try:\n",
        "    from IPython.display import display\n",
        "    bert_acc = accuracy_score(true_seqs, pred_seqs)\n",
        "    bert_macro = f1_score(true_seqs, pred_seqs, average='macro')\n",
        "    bert_micro = f1_score(true_seqs, pred_seqs, average='micro')\n",
        "    res.append({\n",
        "        'Model': 'BERT-base (Token Classification)',\n",
        "        'Accuracy': bert_acc,\n",
        "        'Macro F1': bert_macro,\n",
        "        'Micro F1': bert_micro,\n",
        "        'Params (~)': '110M',\n",
        "        'Complexity': 'O(T^2) self-attn'\n",
        "    })\n",
        "except Exception as e:\n",
        "    print('BERT metrics not available yet:', e)\n",
        "\n",
        "if res:\n",
        "    df = pd.DataFrame(res)\n",
        "    display(df)\n",
        "else:\n",
        "    print('Run the training/evaluation cells above first to populate results.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìù Discussion\n",
        "**Why contextual embeddings and attention help:**\n",
        "- Contextual models (ELMo/BERT) encode **polysemy**: the representation of a word depends on its **surrounding words**, improving disambiguation (e.g., *book* as NOUN vs VERB).\n",
        "- Attention mechanisms allow the model to **focus on the most informative context tokens** for each position (e.g., determiners, auxiliaries), enhancing long-range dependency modeling beyond local windows.\n",
        "\n",
        "**Expected ranges on UD English (indicative):**\n",
        "- Static BiLSTM: ~95‚Äì97% token accuracy\n",
        "- BiLSTM + Attention: ~96‚Äì97.5%\n",
        "- BERT-base: ~97.5‚Äì98.5%\n",
        "\n",
        "Your exact numbers will vary by preprocessing, hyperparameters, and random seed.\n",
        "\n",
        "---\n",
        "\n",
        "## üìö References\n",
        "- Vaswani et al. (2017). *Attention is All You Need.*\n",
        "- Peters et al. (2018). *Deep contextualized word representations (ELMo).* \n",
        "- Devlin et al. (2019). *BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding.*\n",
        "- Ma & Hovy (2016). *End-to-end Sequence Labeling via BiLSTM-CNNs-CRF.*\n",
        "- UD English EWT Treebank: https://universaldependencies.org/\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
